{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e897d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# --- 하이퍼파라미터 설정 ---\n",
    "# 실험 환경 설정\n",
    "TOTAL_TRIALS = 100\n",
    "REWARD_BUDGET_PER_ARM = 25\n",
    "TARGET_ACTION = 0  # 목표 행동 (0: 왼쪽, 1: 오른쪽)\n",
    "\n",
    "# QLearner 설정\n",
    "LEARNER_ALPHA = 0.1  # 학습률\n",
    "LEARNER_GAMMA = 0.9  # 할인율\n",
    "LEARNER_EPSILON = 0.1 # 탐험 확률\n",
    "\n",
    "# LearnerModel (RNN) 설정\n",
    "MODEL_HIDDEN_SIZE = 8\n",
    "MODEL_EPOCHS = 300\n",
    "MODEL_LR = 0.005\n",
    "MODEL_TRAINING_SAMPLES = 500\n",
    "\n",
    "# Adversary (DQN) 설정\n",
    "ADV_HIDDEN_SIZE = 128\n",
    "ADV_BATCH_SIZE = 64\n",
    "ADV_GAMMA = 0.99\n",
    "ADV_EPS_START = 0.9\n",
    "ADV_EPS_END = 0.05\n",
    "ADV_EPS_DECAY = 1000\n",
    "ADV_TAU = 0.005\n",
    "ADV_LR = 1e-4\n",
    "ADV_TRAINING_EPISODES = 500\n",
    "\n",
    "# 평가 설정\n",
    "EVALUATION_EPISODES = 100\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. Learner Agent: Q-Learning 기반 Bandit 에이전트 ---\n",
    "class QLearner:\n",
    "    \"\"\"간단한 Q-learning 에이전트. Adversary의 공격 대상.\"\"\"\n",
    "    def __init__(self, alpha, gamma, epsilon):\n",
    "        self.q_table = np.zeros(2)  # [Q(action_0), Q(action_1)]\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.last_action = None\n",
    "\n",
    "    def choose_action(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice([0, 1])\n",
    "        else:\n",
    "            action = np.argmax(self.q_table)\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def update(self, reward):\n",
    "        old_value = self.q_table[self.last_action]\n",
    "        # Bandit 문제는 상태가 없으므로, 다음 상태의 가치는 0으로 간주\n",
    "        next_max = 0\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "        self.q_table[self.last_action] = new_value\n",
    "\n",
    "# --- 2. Learner Model: Learner의 행동을 모방하는 RNN ---\n",
    "class LearnerModel(nn.Module):\n",
    "    \"\"\"Learner 에이전트의 행동을 모방하는 GRU 모델.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LearnerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return F.softmax(out, dim=1), h\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# --- 3. Adversary Agent: 보상 전략을 학습하는 DQN 에이전트 ---\n",
    "ReplayMemory = namedtuple('ReplayMemory', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"DQN을 위한 리플레이 버퍼.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(ReplayMemory(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class AdversaryDQN(nn.Module):\n",
    "    \"\"\"보상 제공 전략을 학습하는 DQN 모델.\"\"\"\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(AdversaryDQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, ADV_HIDDEN_SIZE)\n",
    "        self.layer2 = nn.Linear(ADV_HIDDEN_SIZE, ADV_HIDDEN_SIZE)\n",
    "        self.layer3 = nn.Linear(ADV_HIDDEN_SIZE, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "# --- 훈련 및 평가 로직 ---\n",
    "\n",
    "def train_learner_model(learner_model, q_learner):\n",
    "    \"\"\"QLearner의 데이터를 생성하고, 이를 이용해 LearnerModel을 훈련.\"\"\"\n",
    "    print(\"--- 1단계: LearnerModel 훈련 시작 ---\")\n",
    "    optimizer = optim.Adam(learner_model.parameters(), lr=MODEL_LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 데이터 생성\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for _ in range(MODEL_TRAINING_SAMPLES):\n",
    "        learner = copy.deepcopy(q_learner)\n",
    "        hidden = learner_model.init_hidden()\n",
    "        for _ in range(TOTAL_TRIALS):\n",
    "            # 랜덤 환경에서 행동 및 학습\n",
    "            action = learner.choose_action()\n",
    "            reward = 1 if random.random() < 0.5 else 0\n",
    "            \n",
    "            # 이전 행동과 보상을 입력으로, 현재 행동을 타겟으로 설정\n",
    "            # 입력 형태: [이전 행동, 이전 보상]\n",
    "            # one-hot encoding: action 0 -> [1,0], action 1 -> [0,1]\n",
    "            prev_action_vec = [1, 0] if learner.last_action == 0 else [0, 1]\n",
    "            prev_reward = [reward]\n",
    "            \n",
    "            # 첫 trial은 이전 정보가 없으므로 스킵\n",
    "            if learner.last_action is not None:\n",
    "                model_input = torch.tensor([prev_action_vec + prev_reward], dtype=torch.float32)\n",
    "                inputs.append(model_input)\n",
    "                targets.append(action)\n",
    "\n",
    "            learner.update(reward)\n",
    "\n",
    "    # LearnerModel 훈련\n",
    "    for epoch in range(MODEL_EPOCHS):\n",
    "        total_loss = 0\n",
    "        for i in range(len(inputs)):\n",
    "            optimizer.zero_grad()\n",
    "            hidden = learner_model.init_hidden() # 매 샘플마다 hidden state 초기화\n",
    "            \n",
    "            # 모델 입력 형태: (batch, seq_len, input_size)\n",
    "            model_input = inputs[i].unsqueeze(0).to(device)\n",
    "            target = torch.tensor([targets[i]], dtype=torch.long).to(device)\n",
    "            \n",
    "            output, hidden = learner_model(model_input, hidden)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{MODEL_EPOCHS}, Loss: {total_loss/len(inputs):.4f}\")\n",
    "    print(\"--- LearnerModel 훈련 완료 ---\\n\")\n",
    "\n",
    "\n",
    "def train_adversary(adversary_policy_net, adversary_target_net, learner_model):\n",
    "    \"\"\"LearnerModel을 상대로 Adversary를 훈련.\"\"\"\n",
    "    print(\"--- 2단계: Adversary 훈련 시작 ---\")\n",
    "    optimizer = optim.Adam(adversary_policy_net.parameters(), lr=ADV_LR)\n",
    "    memory = ReplayBuffer(10000)\n",
    "    steps_done = 0\n",
    "\n",
    "    def select_adversary_action(state):\n",
    "        nonlocal steps_done\n",
    "        sample = random.random()\n",
    "        eps_threshold = ADV_EPS_END + (ADV_EPS_START - ADV_EPS_END) * \\\n",
    "            np.exp(-1. * steps_done / ADV_EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return adversary_policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            # Adversary 행동: 0 (왼쪽 보상), 1 (오른쪽 보상)\n",
    "            return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "\n",
    "    def optimize_model():\n",
    "        if len(memory) < ADV_BATCH_SIZE:\n",
    "            return\n",
    "        transitions = memory.sample(ADV_BATCH_SIZE)\n",
    "        batch = ReplayMemory(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        state_action_values = adversary_policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_state_values = torch.zeros(ADV_BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = adversary_target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "        expected_state_action_values = (next_state_values * ADV_GAMMA) + reward_batch\n",
    "\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(adversary_policy_net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    for i_episode in range(ADV_TRAINING_EPISODES):\n",
    "        rewards_left = [REWARD_BUDGET_PER_ARM, REWARD_BUDGET_PER_ARM]\n",
    "        learner_model_hidden = learner_model.init_hidden()\n",
    "        \n",
    "        # Adversary의 state: [learner_hidden_state, trial_num, r_left_0, r_left_1]\n",
    "        state = torch.cat([\n",
    "            learner_model_hidden.view(-1), \n",
    "            torch.tensor([0, rewards_left[0], rewards_left[1]], dtype=torch.float32, device=device)\n",
    "        ]).unsqueeze(0)\n",
    "        \n",
    "        last_learner_action = 0\n",
    "        last_reward_to_learner = 0\n",
    "\n",
    "        for t in range(TOTAL_TRIALS):\n",
    "            # 1. Adversary가 보상 할당 결정\n",
    "            adv_action_tensor = select_adversary_action(state)\n",
    "            adv_action = adv_action_tensor.item()\n",
    "            \n",
    "            # 예산 제약 조건 강제 적용\n",
    "            # 만약 예산이 없으면, 반대편에 강제 할당\n",
    "            if rewards_left[adv_action] == 0:\n",
    "                adv_action = 1 - adv_action\n",
    "            # 만약 남은 trial 수와 예산 수가 같으면, 남은 기간 동안 계속 보상 강제 할당\n",
    "            trials_left = TOTAL_TRIALS - t\n",
    "            if rewards_left[0] == trials_left: adv_action = 0\n",
    "            if rewards_left[1] == trials_left: adv_action = 1\n",
    "\n",
    "            reward_assignment = [0, 0]\n",
    "            reward_assignment[adv_action] = 1\n",
    "            rewards_left[adv_action] -= 1\n",
    "\n",
    "            # 2. LearnerModel이 행동 결정\n",
    "            prev_action_vec = [1, 0] if last_learner_action == 0 else [0, 1]\n",
    "            model_input = torch.tensor([[prev_action_vec + [last_reward_to_learner]]], dtype=torch.float32).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action_probs, next_learner_model_hidden = learner_model(model_input, learner_model_hidden)\n",
    "            \n",
    "            learner_action = torch.multinomial(action_probs, 1).item()\n",
    "            reward_to_learner = reward_assignment[learner_action]\n",
    "\n",
    "            # 3. Adversary에 대한 보상 계산\n",
    "            adversary_reward = 1.0 if learner_action == TARGET_ACTION else 0.0\n",
    "            adversary_reward = torch.tensor([adversary_reward], device=device)\n",
    "            \n",
    "            # 4. 다음 상태 준비 및 메모리에 저장\n",
    "            last_learner_action = learner_action\n",
    "            last_reward_to_learner = reward_to_learner\n",
    "            learner_model_hidden = next_learner_model_hidden\n",
    "\n",
    "            if t == TOTAL_TRIALS - 1:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.cat([\n",
    "                    learner_model_hidden.view(-1), \n",
    "                    torch.tensor([t+1, rewards_left[0], rewards_left[1]], dtype=torch.float32, device=device)\n",
    "                ]).unsqueeze(0)\n",
    "\n",
    "            memory.push(state, adv_action_tensor, next_state, adversary_reward)\n",
    "            state = next_state\n",
    "\n",
    "            # 5. DQN 모델 최적화\n",
    "            optimize_model()\n",
    "\n",
    "            # Target network 업데이트\n",
    "            target_net_state_dict = adversary_target_net.state_dict()\n",
    "            policy_net_state_dict = adversary_policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*ADV_TAU + target_net_state_dict[key]*(1-ADV_TAU)\n",
    "            adversary_target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if state is None:\n",
    "                break\n",
    "        \n",
    "        if (i_episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {i_episode+1}/{ADV_TRAINING_EPISODES}\")\n",
    "\n",
    "    print(\"--- Adversary 훈련 완료 ---\\n\")\n",
    "\n",
    "\n",
    "def evaluate(adversary, q_learner_template):\n",
    "    \"\"\"훈련된 Adversary를 실제 QLearner와 대결시켜 평가.\"\"\"\n",
    "    print(\"--- 3단계: 평가 시작 ---\")\n",
    "    target_action_counts = []\n",
    "\n",
    "    for _ in range(EVALUATION_EPISODES):\n",
    "        q_learner = copy.deepcopy(q_learner_template)\n",
    "        rewards_left = [REWARD_BUDGET_PER_ARM, REWARD_BUDGET_PER_ARM]\n",
    "        \n",
    "        # 평가 시에는 LearnerModel이 아닌, Adversary의 state 추적용 '가짜' hidden state 사용\n",
    "        # 실제 Learner의 내부 상태는 알 수 없기 때문 (논문의 closed-loop와 유사한 개념)\n",
    "        # 하지만 이 예제에서는 QLearner의 내부 상태(q_table)를 Adversary의 state로 사용해 단순화\n",
    "        \n",
    "        count = 0\n",
    "        for t in range(TOTAL_TRIALS):\n",
    "            # Adversary의 state: [q_val_0, q_val_1, trial_num, r_left_0, r_left_1]\n",
    "            # LearnerModel의 hidden state 대신 Q-table을 사용\n",
    "            state = torch.tensor([q_learner.q_table[0], q_learner.q_table[1], t, rewards_left[0], rewards_left[1]], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                adv_action = adversary(state).max(1)[1].item()\n",
    "\n",
    "            # 예산 제약 조건 강제 적용\n",
    "            if rewards_left[adv_action] == 0: adv_action = 1 - adv_action\n",
    "            trials_left = TOTAL_TRIALS - t\n",
    "            if rewards_left[0] == trials_left: adv_action = 0\n",
    "            if rewards_left[1] == trials_left: adv_action = 1\n",
    "            \n",
    "            reward_assignment = [0, 0]\n",
    "            reward_assignment[adv_action] = 1\n",
    "            rewards_left[adv_action] -= 1\n",
    "\n",
    "            # QLearner가 행동하고 학습\n",
    "            learner_action = q_learner.choose_action()\n",
    "            reward_to_learner = reward_assignment[learner_action]\n",
    "            q_learner.update(reward_to_learner)\n",
    "\n",
    "            if learner_action == TARGET_ACTION:\n",
    "                count += 1\n",
    "        \n",
    "        target_action_counts.append(count)\n",
    "\n",
    "    # 결과 시각화\n",
    "    bias = (np.mean(target_action_counts) / TOTAL_TRIALS) * 100\n",
    "    print(f\"평가 완료. 평균 목표 행동 선택 비율: {bias:.2f}%\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(target_action_counts, bins=np.arange(0, TOTAL_TRIALS + 2) - 0.5, alpha=0.7, label=f'평균: {np.mean(target_action_counts):.1f}회')\n",
    "    plt.axvline(TOTAL_TRIALS / 2, color='r', linestyle='--', label='무작위 선택 기준 (50회)')\n",
    "    plt.title(f'Adversary 대결 후 목표 행동({TARGET_ACTION}) 선택 횟수 분포', fontsize=16)\n",
    "    plt.xlabel('100회 중 목표 행동 선택 횟수', fontsize=12)\n",
    "    plt.ylabel('에피소드 수', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b48ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_q_learner = QLearner(alpha=LEARNER_ALPHA, gamma=LEARNER_GAMMA, epsilon=LEARNER_EPSILON)\n",
    "learner_model = LearnerModel(input_size=3, hidden_size=MODEL_HIDDEN_SIZE, output_size=2).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b33cd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_state_size = MODEL_HIDDEN_SIZE + 3\n",
    "adv_action_size = 2 # 0: 왼쪽 보상 할당, 1: 오른쪽 보상 할당\n",
    "\n",
    "adversary_policy_net = AdversaryDQN(adv_state_size, adv_action_size).to(device)\n",
    "adversary_target_net = AdversaryDQN(adv_state_size, adv_action_size).to(device)\n",
    "adversary_target_net.load_state_dict(adversary_policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bdbf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1단계: LearnerModel 훈련 시작 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. 훈련 단계 실행\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_learner_model(learner_model, base_q_learner)\n\u001b[32m      3\u001b[39m train_adversary(adversary_policy_net, adversary_target_net, learner_model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mtrain_learner_model\u001b[39m\u001b[34m(learner_model, q_learner)\u001b[39m\n\u001b[32m    162\u001b[39m     loss = criterion(output, target)\n\u001b[32m    163\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     optimizer.step()\n\u001b[32m    165\u001b[39m     total_loss += loss.item()\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % \u001b[32m50\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/motornet/lib/python3.13/site-packages/torch/optim/optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = func(*args, **kwargs)\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/motornet/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/motornet/lib/python3.13/site-packages/torch/optim/adam.py:223\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    211\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    213\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    214\u001b[39m         group,\n\u001b[32m    215\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         state_steps,\n\u001b[32m    221\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     adam(\n\u001b[32m    224\u001b[39m         params_with_grad,\n\u001b[32m    225\u001b[39m         grads,\n\u001b[32m    226\u001b[39m         exp_avgs,\n\u001b[32m    227\u001b[39m         exp_avg_sqs,\n\u001b[32m    228\u001b[39m         max_exp_avg_sqs,\n\u001b[32m    229\u001b[39m         state_steps,\n\u001b[32m    230\u001b[39m         amsgrad=group[\u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    231\u001b[39m         has_complex=has_complex,\n\u001b[32m    232\u001b[39m         beta1=beta1,\n\u001b[32m    233\u001b[39m         beta2=beta2,\n\u001b[32m    234\u001b[39m         lr=group[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    235\u001b[39m         weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    236\u001b[39m         eps=group[\u001b[33m\"\u001b[39m\u001b[33meps\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    237\u001b[39m         maximize=group[\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    238\u001b[39m         foreach=group[\u001b[33m\"\u001b[39m\u001b[33mforeach\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    239\u001b[39m         capturable=group[\u001b[33m\"\u001b[39m\u001b[33mcapturable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    240\u001b[39m         differentiable=group[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    241\u001b[39m         fused=group[\u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    242\u001b[39m         grad_scale=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgrad_scale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    243\u001b[39m         found_inf=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfound_inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    244\u001b[39m     )\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/motornet/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/motornet/lib/python3.13/site-packages/torch/optim/adam.py:784\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    782\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m func(\n\u001b[32m    785\u001b[39m     params,\n\u001b[32m    786\u001b[39m     grads,\n\u001b[32m    787\u001b[39m     exp_avgs,\n\u001b[32m    788\u001b[39m     exp_avg_sqs,\n\u001b[32m    789\u001b[39m     max_exp_avg_sqs,\n\u001b[32m    790\u001b[39m     state_steps,\n\u001b[32m    791\u001b[39m     amsgrad=amsgrad,\n\u001b[32m    792\u001b[39m     has_complex=has_complex,\n\u001b[32m    793\u001b[39m     beta1=beta1,\n\u001b[32m    794\u001b[39m     beta2=beta2,\n\u001b[32m    795\u001b[39m     lr=lr,\n\u001b[32m    796\u001b[39m     weight_decay=weight_decay,\n\u001b[32m    797\u001b[39m     eps=eps,\n\u001b[32m    798\u001b[39m     maximize=maximize,\n\u001b[32m    799\u001b[39m     capturable=capturable,\n\u001b[32m    800\u001b[39m     differentiable=differentiable,\n\u001b[32m    801\u001b[39m     grad_scale=grad_scale,\n\u001b[32m    802\u001b[39m     found_inf=found_inf,\n\u001b[32m    803\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/motornet/lib/python3.13/site-packages/torch/optim/adam.py:430\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    428\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    432\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2. 훈련 단계 실행\n",
    "train_learner_model(learner_model, base_q_learner)\n",
    "train_adversary(adversary_policy_net, adversary_target_net, learner_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motornet",
   "language": "python",
   "name": "motornet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
